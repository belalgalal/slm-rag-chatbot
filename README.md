# SLM RAG Chatbot for (Node.js + Ollama + Local Embeddings)

It combines:
- ğŸ§  A local Small Language Model (e.g. Mistral via [Ollama](https://ollama.com))
- ğŸ§® Custom vector index for semantic similarity
- ğŸ“š Local documents (no internet)
- ğŸ§¬ Real embeddings using [Xenova Transformers](https://xenova.github.io/transformers.js/)

---

## ğŸ“ Project Structure

```
rag-chatbot-node-esm/
â”œâ”€â”€ server.js              # Express API server
â”œâ”€â”€ rag.js                 # Core RAG logic: similarity + prompt
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ embedder.js        # Embedding function (mocked or real)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ hr-faq.md          # Your HR documents
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Install Ollama

Install Ollama and pull a model like Mistral:

```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral
```

### 2. Install Node Modules

```bash
npm install
```

### 3. Start the Server

```bash
npm start
```

---

## ğŸ’¬ Query the Chatbot

```bash
curl -X POST http://localhost:3000/chat \
  -H 'Content-Type: application/json' \
  -d '{"question": "How many vacation days do I have?"}' | jq
```

Output:
```json
{
   "answer": "You have 25 working days of paid vacation per year."
}
```

```bash
curl -X POST http://localhost:3000/chat \
  -H 'Content-Type: application/json' \
  -d '{"question": "Is there a dress code?"}' | jq
```

Output:
```json
{
   "answer": "Yes, there is a dress code. The company follows a smart casual dress code, except for formal client meetings."
}
```


---

## ğŸ§  How It Works

1. Loads and embeds HR documents on startup.
2. On each question:
   - Embeds the question.
   - Finds the most relevant doc using cosine similarity.
   - Builds a prompt: _â€œAnswer the following based on this document...â€_
   - Sends it to the local `mistral` model via Ollama.
   - Returns the model's answer.

---

## ğŸ§° Architecture Diagram

```ascii
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        User / Client          â”‚
                    â”‚   (asks HR-related question)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         Express Server        â”‚
                    â”‚        /chat endpoint         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚         RAG Pipeline (rag.js functions)       â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼                              â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ getEmbedding(text)  â”‚   â”‚ cosineSimilarity(a, b)       â”‚   â”‚ runOllama(prompt)           â”‚
â”‚ (utils/embedder.js) â”‚   â”‚ Compares question/doc vectorsâ”‚   â”‚ Calls local LLM via CLI     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                           â”‚                                  â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Embed question    â”‚     â”‚ Find top document       â”‚       â”‚ Prompt:                  â”‚
   â”‚ â†’ questionVector  â”‚     â”‚ using similarity scoringâ”‚       â”‚ "Answer based on..."     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Answer generated by LLM  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   JSON response to the client  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

â¡ï¸ **RAG (Retrieval-Augmented Generation)**: Combines external context (retrieved documents) with LLMs to generate more accurate answers.

â¡ï¸ **Embedder / Sentence Transformer**: Converts text into numerical vectors that capture semantic meaning.

â¡ï¸ **Vectors**: Arrays of numbers used to represent text for comparison.

â¡ï¸ **Cosine Similarity**: Finds the most relevant document by comparing angles between vectors.

---

## ğŸ§ª Tips

- Use `jq` for pretty-printing JSON responses.
- Extend `hr-faq.md` with your own company info.

---

## ğŸ“Œ Notes

- Fully local: No internet access needed after pulling model.
- Ideal for internal or offline environments.

## ğŸ”® Future Improvements
- Dynamic Document Uploads: add an API to ingest new HR documents at runtime (live uploads).
- Persistent Vector Store: use a database like Qdrant or Weaviate to store vectors across restarts.
- Persistent Document Map: store the doc list in DB to store documents across restarts
- Frontend Chat UI: build a simple React-based interface for non-technical users.
- Multi-document Retrieval: Support retrieving and combining multiple relevant documents.
- Metadata Support: for example, tag documents with titles, authors, or departments for better filtering.
- Authentication: secure the API so only internal users can access it.
- Chat History: store past questions and answers for context or auditing.
- The sky is your limit!
